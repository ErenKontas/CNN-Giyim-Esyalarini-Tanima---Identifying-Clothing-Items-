{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48407da4-8244-4162-a72d-b9fdd04ae3d4",
   "metadata": {
    "id": "48407da4-8244-4162-a72d-b9fdd04ae3d4"
   },
   "source": [
    "# <font color=#025dfa> Giyim Eşyalarını Tanıma - Identifying Clothing Items"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9792c2dd-4d03-4738-a622-40507257bff1",
   "metadata": {
    "id": "9792c2dd-4d03-4738-a622-40507257bff1"
   },
   "source": [
    "TR = Her yorum satırı kendisini üstündeki koda aittir. İlk olarak Türkçe, son olarak İngilizce yazıldı.\n",
    "\n",
    "EN = Each comment line belongs to the code above it. It was first written in Turkish and lastly in English.\n",
    "\n",
    "TR = Giyim eşyalarını görüntüler üzerinden otomatik olarak tanıyan ve sınıflandıran bir model geliştirmeyi amaçlayan bu proje, derin öğrenme ve bilgisayarla görme teknikleri kullanarak farklı giysi türlerini analiz edecektir.\n",
    "\n",
    "EN = Aiming to develop a model that automatically recognizes and classifies clothing items from images, this project will analyze different types of clothing using deep learning and computer vision techniques.\n",
    "\n",
    "Kaynak/Source = https://thecleverprogrammer.com/2020/08/30/image-classification-with-tensorflow-in-machine-learning/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98f8e4eb-7913-4edb-9393-ea64c5d395fc",
   "metadata": {
    "id": "98f8e4eb-7913-4edb-9393-ea64c5d395fc"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Conv2D, InputLayer , Reshape , MaxPooling2D, Flatten, Dropout, BatchNormalization\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.keras.applications import VGG16,ResNet50\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator #Tek satırda resimleri okumayı sağlıyor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "VVaixvoqa_nm",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VVaixvoqa_nm",
    "outputId": "7a860d7e-a9e1-44bc-d882-2332088a596f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
      "\u001b[1m29515/29515\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
      "\u001b[1m26421880/26421880\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
      "\u001b[1m5148/5148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
      "\u001b[1m4422102/4422102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from keras.utils import to_categorical\n",
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "DAefOV4bIBgD",
   "metadata": {
    "id": "DAefOV4bIBgD"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63367a39-0d5d-4d4d-a920-08b76ee4b918",
   "metadata": {
    "id": "63367a39-0d5d-4d4d-a920-08b76ee4b918"
   },
   "outputs": [],
   "source": [
    "labels = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "# TR = Etiketleri saklayacak bir liste oluşturur.\n",
    "# EN = Creates a list to store the labels.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4fdff5-2291-42e0-a77a-8e52fd69b076",
   "metadata": {
    "id": "0b4fdff5-2291-42e0-a77a-8e52fd69b076"
   },
   "source": [
    "## <font color=#FFD700> EDA Keşif Amaçlı Veri Analizi - EDA - Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "GQbmOjQKKoNk",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GQbmOjQKKoNk",
    "outputId": "5bb8555a-4f04-4482-d979-3cca56ed4751"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train= (60000, 28, 28)\n",
      "x_test= (10000, 28, 28)\n",
      "y_train= (60000,)\n",
      "y_test= (10000,)\n"
     ]
    }
   ],
   "source": [
    "print('x_train=',x_train.shape)\n",
    "print('x_test=',x_test.shape)\n",
    "print('y_train=',y_train.shape)\n",
    "print('y_test=',y_test.shape)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d79c8e42-0a91-415a-9d46-1799a3a9f7f8",
   "metadata": {
    "id": "d79c8e42-0a91-415a-9d46-1799a3a9f7f8"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfdccf87-1474-4438-9991-1b7207457203",
   "metadata": {},
   "outputs": [],
   "source": [
    "for label_index, label_name in enumerate(labels):\n",
    "    # TR = İlgili etiketle eşleşen tüm görüntü yolları bir listeye alınır  \n",
    "    # EN = All image paths matching the label are retrieved into a list\n",
    "    label_images = x_train[y_train == label_index]  \n",
    "    \n",
    "    if len(label_images) >= 9:  \n",
    "        # TR = Eğer görüntü sayısı 9 veya daha fazlaysa işlem devam eder  \n",
    "        # EN = Proceed if there are 9 or more images\n",
    "\n",
    "        selected_images = random.sample(list(label_images), 9)  \n",
    "        # TR = Görüntüler arasından rastgele 9 tanesi seçilir  \n",
    "        # EN = Randomly select 9 images from the list\n",
    "        \n",
    "        plt.figure(figsize=(15, 15))  \n",
    "        # TR = 15x15 boyutlarında bir figür oluşturulur  \n",
    "        # EN = Create a figure of size 15x15\n",
    "        \n",
    "        for i, img in enumerate(selected_images):  \n",
    "            # TR = Seçilen her bir görüntü için döngü başlatılır  \n",
    "            # EN = Loop through each selected image\n",
    "            \n",
    "            plt.subplot(3, 3, i + 1)  \n",
    "            # TR = 3x3'lük bir yerleşimde görüntü konumlandırılır  \n",
    "            # EN = Place the image in a 3x3 grid\n",
    "            \n",
    "            plt.imshow(img, cmap='gray')  \n",
    "            # TR = Görüntü ekranda gösterilir  \n",
    "            # EN = Display the image in grayscale\n",
    "            \n",
    "            plt.title(label_name)  \n",
    "            # TR = Her görüntüye ait başlık olarak etiket eklenir  \n",
    "            # EN = Set the label as the title for each image\n",
    "            \n",
    "            plt.axis('off')  \n",
    "            # TR = Eksenler gizlenir  \n",
    "            # EN = Hide the axes\n",
    "        \n",
    "        plt.show()  \n",
    "        # TR = Görüntüler ekranda gösterilir  \n",
    "        # EN = Display the images on the screen\n",
    "        \n",
    "    else:  \n",
    "        # TR = Eğer yeterli görüntü yoksa uyarı verir  \n",
    "        # EN = If not enough images are available, show a warning message\n",
    "        print(f\"Yeterli görüntü bulunamadı/Not enough images found: {label_name}\")  \n",
    "        # TR = Ekranda yeterli görüntü bulunamadığını belirtir  \n",
    "        # EN = Print a message that not enough images were found"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9e0f1417-5460-4c9b-809e-a48bb2c10beb",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5tOLINCOLe2i",
   "metadata": {
    "id": "5tOLINCOLe2i"
   },
   "outputs": [],
   "source": [
    "x_train=x_train/255.0\n",
    "x_test=x_test/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "NIFgrcSXLfNC",
   "metadata": {
    "id": "NIFgrcSXLfNC"
   },
   "outputs": [],
   "source": [
    "y_train = to_categorical(y_train, 10)\n",
    "y_test = to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "64c6e593-2ab1-43ec-a7a7-86cdec129c09",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "094af6b1-1921-427f-8086-7216a43966c5",
   "metadata": {
    "id": "094af6b1-1921-427f-8086-7216a43966c5"
   },
   "source": [
    "## <font color='#0F52BA'> Öznitelik Mühendisliği - Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6ece7d-bdf5-4382-81b3-860edaa5f25e",
   "metadata": {
    "id": "7f6ece7d-bdf5-4382-81b3-860edaa5f25e"
   },
   "source": [
    "### <font color=#007fff> Model - Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "216c8b0c-24b5-4631-be6a-d2f5a0ee59f1",
   "metadata": {
    "id": "216c8b0c-24b5-4631-be6a-d2f5a0ee59f1"
   },
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test=train_test_split(x_train,y_train,test_size=.20,random_state=42)\n",
    "# TR = modelimizi eğittik.\n",
    "# EN = We trained our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b9921796-40cb-4256-8a17-efeb147dc05a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b9921796-40cb-4256-8a17-efeb147dc05a",
    "outputId": "e334a1e9-248a-43f1-dfa6-d02f004e910d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m1350/1350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 3ms/step - accuracy: 0.7638 - loss: 0.7150 - val_accuracy: 0.8492 - val_loss: 0.4170\n",
      "Epoch 2/10\n",
      "\u001b[1m1350/1350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.8673 - loss: 0.3781 - val_accuracy: 0.8698 - val_loss: 0.3556\n",
      "Epoch 3/10\n",
      "\u001b[1m1350/1350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.8839 - loss: 0.3355 - val_accuracy: 0.8794 - val_loss: 0.3345\n",
      "Epoch 4/10\n",
      "\u001b[1m1350/1350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8918 - loss: 0.3088 - val_accuracy: 0.8854 - val_loss: 0.3184\n",
      "Epoch 5/10\n",
      "\u001b[1m1350/1350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.8976 - loss: 0.2893 - val_accuracy: 0.8929 - val_loss: 0.3083\n",
      "Epoch 6/10\n",
      "\u001b[1m1350/1350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9062 - loss: 0.2689 - val_accuracy: 0.8925 - val_loss: 0.3060\n",
      "Epoch 7/10\n",
      "\u001b[1m1350/1350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - accuracy: 0.9089 - loss: 0.2620 - val_accuracy: 0.8950 - val_loss: 0.2957\n",
      "Epoch 8/10\n",
      "\u001b[1m1350/1350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9120 - loss: 0.2532 - val_accuracy: 0.8967 - val_loss: 0.2936\n",
      "Epoch 9/10\n",
      "\u001b[1m1350/1350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9149 - loss: 0.2425 - val_accuracy: 0.8963 - val_loss: 0.2896\n",
      "Epoch 10/10\n",
      "\u001b[1m1350/1350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9148 - loss: 0.2400 - val_accuracy: 0.8952 - val_loss: 0.2887\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7d816a0fb940>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size=28\n",
    "model = Sequential()\n",
    "# TR = Ardışık bir model oluşturur, katmanlar sıralı olarak eklenir.\n",
    "# EN = Creates a sequential model where layers are added in a linear stack.\n",
    "\n",
    "model.add(InputLayer(input_shape=(size,size,1)))\n",
    "# TR = Modelin giriş katmanını tanımlar ve veri boyutunu belirtir.\n",
    "# EN = Defines the input layer of the model and specifies the data dimensions.\n",
    "\n",
    "model.add(Reshape(target_shape=(size,size,1)))\n",
    "# TR = Veriyi 28x28x1 şeklinde yeniden düzenler, gri tonlama görüntüleri için tek bir renk kanalını belirtir.\n",
    "# EN = Reshapes the data to 28x28x1, specifying a single color channel for grayscale images.\n",
    "\n",
    "model.add(Conv2D(filters=12, kernel_size=(3,3), activation='relu'))\n",
    "# TR = Konvolüsyon işlemi ile özellikleri çıkarır, filtreler görüntüdeki desenleri öğrenir ve 'relu' aktivasyon fonksiyonu ile doğrusal olmayan ilişkileri modellemesini sağlar.\n",
    "# EN = Extracts features via convolution, where filters learn patterns in the image and the 'relu' activation function introduces non-linearity.\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "# TR = Özellik haritasını küçültür ve en belirgin özellikleri seçer, böylece hesaplama maliyeti azalır ve modelin genelleştirme yeteneği artar.\n",
    "# EN = Reduces the feature map size and selects the most prominent features, reducing computation and improving the model’s ability to generalize.\n",
    "\n",
    "model.add(Flatten())\n",
    "# TR = Çok boyutlu veri kümesini tek boyutlu bir vektöre dönüştürür, tam bağlantılı katmanlarla işlem için uygun hale getirir.\n",
    "# EN = Converts the multi-dimensional feature map into a one-dimensional vector to be processed by fully connected layers.\n",
    "\n",
    "model.add(Dense(10))\n",
    "# TR = Sonuçları 10 farklı sınıfa dönüştüren bir tam bağlantılı katman ekler, bu da modelin sınıflandırma yapmasını sağlar.\n",
    "# EN = Adds a fully connected layer with 10 units to output results for 10 different classes, enabling classification.\n",
    "\n",
    "model.compile(optimizer='adam', loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n",
    "# TR = Modeli 'adam' optimizasyonu ile derler, bu optimizasyon algoritması öğrenmeyi iyileştirir, ve 'SparseCategoricalCrossentropy' kayıp fonksiyonunu kullanarak çok sınıflı sınıflandırma yapar, doğruluk metriğiyle model performansını değerlendirir.\n",
    "# EN = Compiles the model with the 'adam' optimizer, which improves learning, uses 'SparseCategoricalCrossentropy' loss function for multi-class classification, and evaluates performance with accuracy metric.\n",
    "\n",
    "model.fit(x_train, y_train, validation_split=.10, epochs=10)\n",
    "# TR = Modeli eğitim verileri (train_images) ve etiketleri (train_labels) ile eğitir, %10'luk bir doğrulama seti kullanır ve 10 dönem boyunca eğitir.\n",
    "# EN = Trains the model with the training data (train_images) and labels (train_labels), uses 10% of the data for validation, and trains for 10 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "17d93b9a-09ef-4eab-8043-c1d6e04a8067",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 305
    },
    "id": "17d93b9a-09ef-4eab-8043-c1d6e04a8067",
    "outputId": "211baf54-37fa-420d-966f-cf5bc3c1b5ae"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ reshape (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">120</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>)          │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2028</span>)                │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)                  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">20,290</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ reshape (\u001b[38;5;33mReshape\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m1\u001b[0m)           │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m12\u001b[0m)          │             \u001b[38;5;34m120\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m12\u001b[0m)          │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2028\u001b[0m)                │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)                  │          \u001b[38;5;34m20,290\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">61,232</span> (239.19 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m61,232\u001b[0m (239.19 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">20,410</span> (79.73 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m20,410\u001b[0m (79.73 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">40,822</span> (159.46 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m40,822\u001b[0m (159.46 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "74994eb6-1fc7-42f5-93fb-d952c991fe7e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "74994eb6-1fc7-42f5-93fb-d952c991fe7e",
    "outputId": "8e670c81-0338-4502-83e8-f7a80d1c0d2b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9006 - loss: 0.2761\n"
     ]
    }
   ],
   "source": [
    "loss, _accuracy = model.evaluate(x_test, y_test)\n",
    "# TR = Test verileri (test_images) ve etiketleri (test_labels) ile modelin performansını değerlendirir, kayıp ve doğruluk değerlerini döndürür.\n",
    "# EN = Evaluates the model's performance using test data (test_images) and labels (test_labels), returning loss and accuracy values."
   ]
  },
  {
   "cell_type": "raw",
   "id": "acc9c653-3f46-4695-8940-b23302e42d5c",
   "metadata": {
    "id": "acc9c653-3f46-4695-8940-b23302e42d5c"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "09ae461a-0d10-49c6-a2bb-f94a7dd05f84",
   "metadata": {
    "id": "09ae461a-0d10-49c6-a2bb-f94a7dd05f84"
   },
   "source": [
    "## <font color='#ec11f7'> Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "-QuoXRKk89GX",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-QuoXRKk89GX",
    "outputId": "1aed25dc-0354-4922-b3bc-cf1349f63114"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "\u001b[1m58889256/58889256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
      "Epoch 1/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 9ms/step - accuracy: 0.7723 - loss: 0.6346 - val_accuracy: 0.8438 - val_loss: 0.4217\n",
      "Epoch 2/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 9ms/step - accuracy: 0.8459 - loss: 0.4112 - val_accuracy: 0.8543 - val_loss: 0.3863\n",
      "Epoch 3/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 9ms/step - accuracy: 0.8640 - loss: 0.3679 - val_accuracy: 0.8637 - val_loss: 0.3689\n",
      "Epoch 4/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 9ms/step - accuracy: 0.8751 - loss: 0.3367 - val_accuracy: 0.8658 - val_loss: 0.3622\n",
      "Epoch 5/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 9ms/step - accuracy: 0.8827 - loss: 0.3121 - val_accuracy: 0.8671 - val_loss: 0.3577\n",
      "Epoch 6/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 9ms/step - accuracy: 0.8873 - loss: 0.2978 - val_accuracy: 0.8728 - val_loss: 0.3457\n",
      "Epoch 7/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 9ms/step - accuracy: 0.8926 - loss: 0.2827 - val_accuracy: 0.8720 - val_loss: 0.3568\n",
      "Epoch 8/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 9ms/step - accuracy: 0.9008 - loss: 0.2660 - val_accuracy: 0.8742 - val_loss: 0.3551\n",
      "Epoch 9/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 9ms/step - accuracy: 0.9052 - loss: 0.2529 - val_accuracy: 0.8727 - val_loss: 0.3510\n",
      "Epoch 10/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 9ms/step - accuracy: 0.9123 - loss: 0.2341 - val_accuracy: 0.8683 - val_loss: 0.3777\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - accuracy: 0.8617 - loss: 0.3932\n",
      "Test Loss: 0.40256574749946594, Test Accuracy: 0.8600000143051147\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
    "# TR = Fashion MNIST veri setini yüklüyoruz. Veriler eğitim ve test olarak ikiye ayrılır.\n",
    "# EN = We load the Fashion MNIST dataset. The data is split into training and testing sets.\n",
    "\n",
    "train_images = np.stack([train_images] * 3, axis=-1)\n",
    "test_images = np.stack([test_images] * 3, axis=-1)\n",
    "# TR = Gri tonlamalı görüntüleri üç kanal olarak yığınlıyoruz, böylece görüntüler RGB formatında olur.\n",
    "# EN = We stack grayscale images into three channels, making the images in RGB format.\n",
    "\n",
    "size = 32\n",
    "\n",
    "train_images = np.array([cv2.resize(image, (size, size)) for image in train_images])\n",
    "test_images = np.array([cv2.resize(image, (size, size)) for image in test_images])\n",
    "# TR = Görüntüleri yeniden boyutlandırıyoruz, her birini 32x32 piksel boyutuna getiriyoruz.\n",
    "# EN = We resize the images, making each 32x32 pixels.\n",
    "\n",
    "train_images = train_images / 255.0\n",
    "test_images = test_images / 255.0\n",
    "# TR = Görüntüleri normalize ederek piksel değerlerini 0 ile 1 arasında bir aralığa getiriyoruz.\n",
    "# EN = We normalize the images, scaling pixel values between 0 and 1.\n",
    "\n",
    "train_datagen = ImageDataGenerator(validation_split=0.2)\n",
    "# TR = Eğitim verileri için %20 doğrulama verisi ayırarak bir veri üreticisi oluşturuyoruz.\n",
    "# EN = We create a data generator for the training data, reserving 20% for validation.\n",
    "\n",
    "train_datagenerator = train_datagen.flow(train_images, train_labels, batch_size=32, subset='training')\n",
    "# TR = Eğitim verileri üzerinde veri üreticisi oluşturuyoruz.\n",
    "# EN = We create a data generator on the training data.\n",
    "\n",
    "validation_datagenerator = train_datagen.flow(train_images, train_labels, batch_size=32, subset='validation')\n",
    "# TR = Doğrulama verileri üzerinde veri üreticisi oluşturuyoruz.\n",
    "# EN = We create a data generator on the validation data.\n",
    "\n",
    "base_model = VGG16(weights='imagenet', input_shape=(size, size, 3), include_top=False)\n",
    "# TR = VGG16 önceden eğitilmiş modelini kullanıyoruz, son katmanı hariç tutuyoruz.\n",
    "# EN = We use the VGG16 pre-trained model, excluding the top (final) layer.\n",
    "\n",
    "for layer in base_model.layers:\n",
    "  layer.trainable = False\n",
    "# TR = VGG16 modelindeki tüm katmanların eğitimde güncellenmesini devre dışı bırakıyoruz.\n",
    "# EN = We freeze all layers in the VGG16 model to prevent them from being updated during training.\n",
    "\n",
    "model = Sequential()\n",
    "model.add(base_model)\n",
    "\n",
    "model.add(Flatten())\n",
    "# TR = Base modeli ve düzleştirme katmanını modelimize ekliyoruz.\n",
    "# EN = We add the base model and a flattening layer to our model.\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "# TR = Bir tam bağlantılı katman (1024 nöron) ekliyoruz.\n",
    "# EN = We add a fully connected layer with 1024 neurons.\n",
    "\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "# TR = Çıkış katmanını ekliyoruz (10 sınıf için softmax aktivasyonu).\n",
    "# EN = We add the output layer with softmax activation for 10 classes.\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "# TR = Modeli Adam optimizasyonu, sparse categorical crossentropy kaybı ve doğruluk metriği ile derliyoruz.\n",
    "# EN = We compile the model with Adam optimizer, sparse categorical crossentropy loss, and accuracy metric.\n",
    "\n",
    "history=model.fit(train_datagenerator, epochs=10, validation_data=validation_datagenerator)\n",
    "# TR = Modeli 10 epoch boyunca eğitiyoruz, doğrulama verileri ile doğruluğu ölçüyoruz.\n",
    "# EN = We train the model for 10 epochs, measuring accuracy on the validation data.\n",
    "\n",
    "loss, accuracy = model.evaluate(test_images, test_labels)\n",
    "print(f'Test Loss: {loss}, Test Accuracy: {accuracy}')\n",
    "# TR = Test verileri üzerinde modelin kaybını ve doğruluğunu değerlendiriyoruz.\n",
    "# EN = We evaluate the model's loss and accuracy on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81493f8b-8bf2-4dc2-b704-21bfe2f50f32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "_ZSzRFu1KRKo",
   "metadata": {
    "id": "_ZSzRFu1KRKo"
   },
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'],label='Accuracy')\n",
    "plt.plot(history.history['val_accuracy'],label='Val_Accuracy')\n",
    "plt.legend();"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
